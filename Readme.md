# ğŸš€ Simple Cloud Storage (SCS)
### *A Production-Grade Amazon S3 Clone - Backend Engineering Capstone Project*

[![Node.js](https://img.shields.io/badge/Node.js-18+-green.svg)](https://nodejs.org/)
[![MongoDB](https://img.shields.io/badge/MongoDB-6.0+-brightgreen.svg)](https://mongodb.com/)
[![Redis](https://img.shields.io/badge/Redis-7.0+-red.svg)](https://redis.io/)
[![SeaweedFS](https://img.shields.io/badge/SeaweedFS-Distributed-blue.svg)](https://github.com/seaweedfs/seaweedfs)
[![License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)

---

## ğŸ“‹ Table of Contents
- [ğŸ¯ Project Overview](#-project-overview)
- [âœ¨ Key Features](#-key-features)
- [ğŸ—ï¸ System Architecture](#-system-architecture)
- [ğŸ› ï¸ Technology Stack](#-technology-stack)
- [ğŸ“ Project Structure](#-project-structure)
- [âš¡ Quick Start](#-quick-start)
- [ğŸ”§ Installation & Setup](#-installation--setup)
- [ğŸ“š API Documentation](#-api-documentation)
- [ğŸ§ª Usage Examples](#-usage-examples)
- [ğŸ” Security Features](#-security-features)
- [ğŸ³ Docker Deployment](#-docker-deployment)
- [ğŸ“Š Monitoring & Analytics](#-monitoring--analytics)

---

## ğŸ¯ Project Overview

**Simple Cloud Storage (SCS)** is a comprehensive Amazon S3 clone built as a capstone project for the Backend Engineering Launchpad program. This project demonstrates deep understanding of distributed systems, cloud storage architecture, and modern backend development practices.

### ğŸ“ **Educational Goals**
- Master **distributed file storage** principles
- Implement **scalable microservices** architecture  
- Learn **cloud storage internals** (metadata, versioning, ACLs)
- Practice **system design** for high-availability services
- Build **production-ready** backend systems

### ğŸŒŸ **Key Achievements**
- âœ… **Full S3 API Compatibility** - Upload, download, versioning, metadata
- âœ… **Enterprise Security** - JWT auth, ACLs, encrypted storage
- âœ… **Production Scalability** - Horizontal scaling, load balancing
- âœ… **Advanced Features** - Deduplication, event streaming, analytics
- âœ… **Modern DevOps** - Docker, NGINX, PM2 clustering

---

## âœ¨ Key Features

<div align="center">

| Feature | Description | Status |
|---------|-------------|--------|
| ğŸ—‚ï¸ **Bucket Management** | Create, list, and manage storage buckets | âœ… Complete |
| ğŸ“ **File Operations** | Upload, download, delete with metadata | âœ… Complete |
| ğŸ”„ **Version Control** | Automatic file versioning and rollback | âœ… Complete |
| ğŸ”— **File Sharing** | Public links with expiration control | âœ… Complete |
| ğŸ” **Access Control** | Granular ACL permissions (Read/Write/Owner) | âœ… Complete |
| ğŸš« **Deduplication** | SHA-256 based duplicate detection | âœ… Complete |
| ğŸ“Š **Event Logging** | Real-time Kafka event streaming | âœ… Complete |
| ğŸŒ **HTTPS Support** | NGINX reverse proxy with SSL | âœ… Complete |
| âš¡ **High Performance** | PM2 clustering and Redis caching | âœ… Complete |

</div>

---

## ğŸ—ï¸ System Architecture

### High-Level Architecture Diagram

```mermaid
graph TB
    Client[ğŸ‘¥ Client Applications<br/>Web/Mobile/CLI]
    
    subgraph "Load Balancer & Proxy"
        NGINX[ğŸŒ NGINX<br/>Reverse Proxy + SSL]
    end
    
    subgraph "API Layer"
        PM2[âš¡ PM2 Cluster<br/>4x Node.js Instances]
        Express[ğŸš€ Express.js API<br/>REST Endpoints]
    end
    
    subgraph "Storage Layer"
        MongoDB[(ğŸ“Š MongoDB<br/>Metadata & Users)]
        Redis[(ğŸ”´ Redis<br/>Bloom Filter & Cache)]
        SeaweedFS[(ğŸ—„ï¸ SeaweedFS<br/>Distributed File Storage)]
    end
    
    subgraph "Event Streaming"
        Kafka[ğŸ“¡ Apache Kafka<br/>Event Logs & Analytics]
        Consumer[ğŸ“ˆ Kafka Consumer<br/>Analytics & Monitoring]
    end
    
    Client --> NGINX
    NGINX --> PM2
    PM2 --> Express
    Express --> MongoDB
    Express --> Redis
    Express --> SeaweedFS
    Express --> Kafka
    Kafka --> Consumer
    
    style Client fill:#e1f5fe
    style NGINX fill:#f3e5f5
    style Express fill:#e8f5e8
    style MongoDB fill:#fff3e0
    style Redis fill:#ffebee
    style SeaweedFS fill:#f1f8e9
    style Kafka fill:#fce4ec
```

### Data Flow Architecture

```mermaid
sequenceDiagram
    participant C as Client
    participant N as NGINX
    participant A as API Server
    participant R as Redis
    participant M as MongoDB
    participant S as SeaweedFS
    participant K as Kafka
    
    C->>N: 1. Upload Request (HTTPS)
    N->>A: 2. Forward to API Server
    A->>A: 3. JWT Authentication
    A->>R: 4. Check Bloom Filter (Dedup)
    
    alt File is Duplicate
        R-->>A: 5a. Hash Exists
        A-->>C: 5b. Skip Upload
    else File is New
        R-->>A: 5c. Hash Not Found
        A->>S: 6. Store File in SeaweedFS
        S-->>A: 7. Return File URL
        A->>M: 8. Save Metadata
        A->>R: 9. Update Bloom Filter
        A->>K: 10. Log Event
        A-->>C: 11. Success Response
    end
```

---

## ğŸ› ï¸ Technology Stack

<div align="center">

### Backend Technologies
![Node.js](https://img.shields.io/badge/Node.js-339933?style=for-the-badge&logo=nodedotjs&logoColor=white)
![Express.js](https://img.shields.io/badge/Express.js-000000?style=for-the-badge&logo=express&logoColor=white)
![JavaScript](https://img.shields.io/badge/JavaScript-F7DF1E?style=for-the-badge&logo=javascript&logoColor=black)

### Databases & Storage
![MongoDB](https://img.shields.io/badge/MongoDB-47A248?style=for-the-badge&logo=mongodb&logoColor=white)
![Redis](https://img.shields.io/badge/Redis-DC382D?style=for-the-badge&logo=redis&logoColor=white)
![SeaweedFS](https://img.shields.io/badge/SeaweedFS-4B8BBE?style=for-the-badge&logo=files&logoColor=white)

### Infrastructure & DevOps
![NGINX](https://img.shields.io/badge/NGINX-009639?style=for-the-badge&logo=nginx&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white)
![Apache Kafka](https://img.shields.io/badge/Apache_Kafka-231F20?style=for-the-badge&logo=apache-kafka&logoColor=white)
![PM2](https://img.shields.io/badge/PM2-2B037A?style=for-the-badge&logo=pm2&logoColor=white)

</div>

---

## ğŸ“ Project Structure

```
vnihar007-scs/
â”œâ”€â”€ ğŸ“ src/                          # Source code directory
â”‚   â”œâ”€â”€ ğŸ® controllers/              # Business logic layer
â”‚   â”‚   â”œâ”€â”€ authController.js        # User authentication & JWT logic
â”‚   â”‚   â”œâ”€â”€ bucketController.js      # Bucket creation, listing, deletion
â”‚   â”‚   â”œâ”€â”€ fileController.js        # File operations (CRUD)
â”‚   â”‚   â”œâ”€â”€ uploadController.js      # File upload logic to SeaweedFS
â”‚   â”‚   â”œâ”€â”€ aclController.js         # Access control layer
â”‚   â”‚   â”œâ”€â”€ shareLinkController.js   # File sharing & link generation
â”‚   â”‚   â””â”€â”€ analyticsController.js   # Storage usage, user activity, metrics
â”‚
â”‚   â”œâ”€â”€ ğŸ“‹ models/                   # MongoDB schema definitions
â”‚   â”‚   â”œâ”€â”€ User.js                  # User schema
â”‚   â”‚   â”œâ”€â”€ Bucket.js                # Bucket schema
â”‚   â”‚   â”œâ”€â”€ File.js                  # Object metadata schema
â”‚   â”‚   â”œâ”€â”€ ACL.js                   # Access control list schema
â”‚   â”‚   â”œâ”€â”€ ShareLink.js             # Shared link schema
â”‚   â”‚   â””â”€â”€ ActivityLog.js           # Logs of user/file actions
â”‚
â”‚   â”œâ”€â”€ ğŸ›£ï¸ routes/                   # API endpoints
â”‚   â”‚   â”œâ”€â”€ auth.js                  # /api/auth
â”‚   â”‚   â”œâ”€â”€ buckets.js               # /api/buckets
â”‚   â”‚   â”œâ”€â”€ files.js                 # /api/files
â”‚   â”‚   â”œâ”€â”€ upload.js                # /api/upload
â”‚   â”‚   â”œâ”€â”€ acl.js                   # /api/acl
â”‚   â”‚   â”œâ”€â”€ Link.js                  # /api/share
â”‚   â”‚   â””â”€â”€ analytics.js             # /api/analytics
â”‚
â”‚   â”œâ”€â”€ ğŸ›¡ï¸ middleware/               # Express middlewares
â”‚   â”‚   â”œâ”€â”€ auth.js                  # JWT validation
â”‚   â”‚   â””â”€â”€ acl.js                   # ACL checks per resource
â”‚
â”‚   â”œâ”€â”€ ğŸ”§ utils/                    # Utility modules
â”‚   â”‚   â”œâ”€â”€ Token_gen.js             # JWT token generator
â”‚   â”‚   â”œâ”€â”€ redis.js                 # Redis & Bloom setup
â”‚   â”‚   â”œâ”€â”€ bloomFilter.js           # Bloom filter helpers
â”‚   â”‚   â”œâ”€â”€ aclCheck.js              # Fine-grained ACL checker
â”‚   â”‚   â”œâ”€â”€ uploadToSeaweed.js       # SeaweedFS uploader
â”‚   â”‚   â””â”€â”€ kafka/                   # Kafka integrations
â”‚   â”‚       â”œâ”€â”€ kafkaClient.js       # KafkaJS client
â”‚   â”‚       â”œâ”€â”€ ProduceEvent.js      # Kafka producer
â”‚   â”‚       â””â”€â”€ Consumer.js          # Kafka consumer
â”‚
â”‚   â”œâ”€â”€ ğŸŒ services/                 # External service configurations
â”‚   â”‚   â”œâ”€â”€ nginx/                   # Reverse proxy setup
â”‚   â”‚   â”‚   â”œâ”€â”€ nginx.conf           # NGINX config file
â”‚   â”‚   â”‚   â””â”€â”€ ssl/                 # SSL/TLS certs
â”‚   â”‚   â”‚       â”œâ”€â”€ selfsigned.crt   # Self-signed certificate
â”‚   â”‚   â”‚       â””â”€â”€ selfsigned.key   # Private key
â”‚
â”‚   â”œâ”€â”€ ğŸ§ª tests/                    # Test suite
â”‚   â”‚   â”œâ”€â”€ analytics/               # Analytics tests
â”‚   â”‚   â”‚   â””â”€â”€ storageAnalytics.test.js
â”‚   â”‚   â”œâ”€â”€ controllers/             # Controller unit tests
â”‚   â”‚   â”‚   â”œâ”€â”€ fileController.test.js
â”‚   â”‚   â”‚   â””â”€â”€ uploadFiles.test.js
â”‚   â”‚   â””â”€â”€ middleware/              # Middleware tests
â”‚   â”‚       â””â”€â”€ aclMiddleware.test.js
â”‚
â”‚   â””â”€â”€ ğŸš€ index.js                  # Main app entry point
â”‚
â”œâ”€â”€ ğŸ³ docker/                       # Docker environment
â”‚   â”œâ”€â”€ kafka-stack/                 # Kafka container stack
â”‚   â”‚   â””â”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ âš™ï¸ .env.test                     # Test environment variables
â”œâ”€â”€ âš™ï¸ .env.example                  # Sample .env config
â”œâ”€â”€ ğŸ“¦ package.json                  # Node dependencies & scripts
â”œâ”€â”€ ğŸ“– README.md                     # Project documentation
â””â”€â”€ ğŸ“¦ package-lock.json             # Dependency lock file
```

---

## âš¡ Quick Start

### Prerequisites
- **Node.js** 18+ 
- **MongoDB** 6.0+
- **Redis** 7.0+
- **Docker** & **Docker Compose**

### 1ï¸âƒ£ Clone & Install
```bash
git clone https://github.com/vNihar007/simple-cloud-storage.git
cd simple-cloud-storage
npm install
```

### 2ï¸âƒ£ Environment Setup
```bash
# Copy environment template
cp .env.example .env

# Edit with your configurations
nano .env
```

### 3ï¸âƒ£ Start Services
```bash
# Start Kafka stack
docker-compose -f kafka-stack/docker-compose.yml up -d

# Start MongoDB & Redis (if not running)
brew services start mongodb/brew/mongodb-community
brew services start redis

# Start the application
npm run dev
```

### 4ï¸âƒ£ Test the API
```bash
# Health check
curl http://localhost:3000/health

# Expected response
{"status": "OK", "timestamp": "2025-06-06T13:20:00Z"}
```

---

## ğŸ”§ Installation & Setup

### Environment Variables

Create a `.env` file in the project root:

```bash
# Database Configuration
MONGOOSE_URI=mongodb://localhost:27017/scs
REDIS_URL=redis://localhost:6379

# Authentication
JWT_SECRET=your-super-secure-jwt-secret-key-here
JWT_EXPIRES_IN=24h

# Kafka Configuration
KAFKA_BROKERS=localhost:9092
KAFKA_TOPIC=file-events

# SeaweedFS Configuration
SEAWEEDFS_MASTER=localhost:9333
SEAWEEDFS_VOLUME=localhost:8080

# Server Configuration
PORT=3000
NODE_ENV=development

# NGINX Configuration (for production)
NGINX_PORT=443
SSL_CERT_PATH=/path/to/ssl/cert
SSL_KEY_PATH=/path/to/ssl/key
```

### Database Setup

```bash
# Connect to MongoDB
mongosh

# Create database and collections
use scs;
db.createCollection("users");
db.createCollection("buckets");
db.createCollection("files");
db.createCollection("acls");

# Create indexes for better performance
db.files.createIndex({"bucket": 1, "filename": 1});
db.files.createIndex({"hash": 1});
db.acls.createIndex({"resourceId": 1, "userId": 1});
```

### Production Deployment

```bash
# Build for production
npm run build

# Start with PM2 cluster
npm run cluster

# Setup NGINX reverse proxy
sudo cp src/services/nginx/nginx.conf /etc/nginx/sites-available/scs
sudo ln -s /etc/nginx/sites-available/scs /etc/nginx/sites-enabled/
sudo nginx -t && sudo systemctl reload nginx
```

---

## ğŸ“š API Documentation

### Authentication Endpoints

#### Register User
```http
POST /api/auth/register
Content-Type: application/json

{
  "email": "john@example.com",
  "password": "securepassword123",
  "name": "John Doe"
}
```

**Response:**
```json
{
  "success": true,
  "token": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...",
  "user": {
    "id": "65f1a2b3c4d5e6f7g8h9i0j1",
    "email": "john@example.com",
    "name": "John Doe"
  }
}
```

#### Login User
```http
POST /api/auth/login
Content-Type: application/json

{
  "email": "john@example.com",
  "password": "securepassword123"
}
```

### Bucket Management

#### Create Bucket
```http
POST /api/bucket/buckets
Authorization: Bearer <jwt-token>
Content-Type: application/json

{
  "name": "my-project-bucket",
  "description": "Storage for project files",
  "isPublic": false
}
```

#### List Buckets
```http
GET /api/bucket/buckets
Authorization: Bearer <jwt-token>
```

**Response:**
```json
{
  "success": true,
  "buckets": [
    {
      "id": "65f1a2b3c4d5e6f7g8h9i0j1",
      "name": "my-project-bucket",
      "description": "Storage for project files",
      "isPublic": false,
      "createdAt": "2025-06-06T10:30:00Z",
      "fileCount": 15,
      "totalSize": "2.5 MB"
    }
  ]
}
```

### File Operations

#### Upload Files
```http
POST /api/upload/:bucketName/upload
Authorization: Bearer <jwt-token>
Content-Type: multipart/form-data

files: [file1.pdf, file2.jpg]
folder: "documents/reports"
tags: "project,2025,important"
versionReason: "Updated quarterly report"
```

**Response:**
```json
{
  "success": true,
  "message": "2 files uploaded successfully",
  "files": [
    {
      "id": "65f1a2b3c4d5e6f7g8h9i0j1",
      "filename": "report.pdf",
      "originalName": "Q1_Report.pdf",
      "size": 1048576,
      "contentType": "application/pdf",
      "version": "v3",
      "hash": "sha256:a1b2c3d4e5f6...",
      "folder": "documents/reports",
      "tags": ["project", "2025", "important"],
      "uploadedAt": "2025-06-06T14:30:00Z"
    }
  ]
}
```

#### Download File
```http
GET /api/files/:bucketName/:fileId/download
Authorization: Bearer <jwt-token>
```

#### Search Files
```http
GET /api/files/:bucketName/search/query?q=report&tags=project&folder=documents
Authorization: Bearer <jwt-token>
```

### File Versioning

#### List File Versions
```http
GET /api/files/:bucketName/versions/:filename
Authorization: Bearer <jwt-token>
```

**Response:**
```json
{
  "success": true,
  "filename": "report.pdf",
  "versions": [
    {
      "version": "v3",
      "size": 1048576,
      "hash": "sha256:a1b2c3d4e5f6...",
      "reason": "Updated quarterly report",
      "uploadedAt": "2025-06-06T14:30:00Z",
      "isCurrent": true
    },
    {
      "version": "v2",
      "size": 987654,
      "hash": "sha256:b2c3d4e5f6a1...",
      "reason": "Added financial data",
      "uploadedAt": "2025-06-05T10:15:00Z",
      "isCurrent": false
    }
  ]
}
```

#### Rollback to Previous Version
```http
POST /api/files/:bucketName/:filename/version/:versionLabel/rollback
Authorization: Bearer <jwt-token>
```

### File Sharing

#### Create Public Share Link
```http
POST /api/link/:fileId/:bucketName/share
Authorization: Bearer <jwt-token>
Content-Type: application/json

{
  "expiresInMinutes": 60,
  "description": "Sharing Q1 report"
}
```

**Response:**
```json
{
  "success": true,
  "shareLink": "https://yourdomain.com/public/my-project-bucket/abc123-def456-ghi789",
  "expiresAt": "2025-06-06T15:30:00Z",
  "accessCount": 0
}
```

### Access Control (ACL)

#### Grant Access Permission
```http
POST /api/acl/assign-owner
Authorization: Bearer <jwt-token>
Content-Type: application/json

{
  "resourceId": "65f1a2b3c4d5e6f7g8h9i0j1",
  "resourceType": "file",
  "userEmail": "collaborator@example.com",
  "permission": "read"
}
```

---

## ğŸ§ª Usage Examples

### Example 1: Complete File Upload Workflow

```bash
#!/bin/bash

# 1. Register a new user
curl -X POST http://localhost:3000/api/auth/register \
  -H "Content-Type: application/json" \
  -d '{
    "email": "demo@example.com",
    "password": "password123",
    "name": "Demo User"
  }'

# 2. Login and get JWT token
TOKEN=$(curl -X POST http://localhost:3000/api/auth/login \
  -H "Content-Type: application/json" \
  -d '{
    "email": "demo@example.com",
    "password": "password123"
  }' | jq -r '.token')

# 3. Create a bucket
curl -X POST http://localhost:3000/api/bucket/buckets \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "demo-bucket",
    "description": "Demo bucket for testing"
  }'

# 4. Upload a file
curl -X POST http://localhost:3000/api/upload/demo-bucket/upload \
  -H "Authorization: Bearer $TOKEN" \
  -F "files=@./sample.pdf" \
  -F "folder=documents" \
  -F "tags=demo,test"

# 5. List bucket contents
curl -X GET http://localhost:3000/api/bucket/buckets/demo-bucket/objects \
  -H "Authorization: Bearer $TOKEN"
```
---

### ğŸ“ Example 2 File Deduplication Logic

> **Goal:** Upload two files with identical content and prevent redundant storage via SHA-256 + Bloom Filter check.

#### ğŸ”„ Flow Overview

```mermaid
sequenceDiagram
    participant U as ğŸ§‘â€ğŸ’» User
    participant API as ğŸš€ Upload API
    participant REDIS as ğŸ” Bloom Filter
    participant DB as ğŸ§¬ MongoDB
    participant FS as ğŸ—‚ï¸ SeaweedFS

    U->>API: Upload file1.txt
    API->>REDIS: Check SHA-256 in Bloom
    REDIS-->>API: Not found
    API->>FS: Store File
    API->>DB: Save Metadata
    API-->>U: âœ… Upload Success

    U->>API: Upload file2.txt (same content)
    API->>REDIS: Check SHA-256
    REDIS-->>API: Possibly found
    API->>DB: Check for confirmed duplicate
    DB-->>API: Duplicate exists
    API-->>U: âš ï¸ Skip Upload, return existing reference
```

#### âœ… Whatâ€™s Being Validated:

* SHA-256 hash is reused to detect identical content.
* Bloom filter offers a **fast, memory-efficient** check.
* MongoDB stores metadata like file ID, owner, timestamps.

#### ğŸ§ª Run Test

```bash
# Run the deduplication test script
node test-deduplication.js
```

> **Tip:** The second upload should be skipped and reference the original file.
> ğŸ” Check terminal logs for `Second upload (should be skipped):` to verify.

---

### ğŸ“¦ Example 3: Bulk Download via Node.js

> **Goal:** Fetch multiple files at once and download them as a single `.zip`.

#### ğŸ“¥ Workflow Overview

```mermaid
graph TD
    A[Select Files for Download] --> B[Send POST Request<br/>/api/files/:bucket/download/bulk]
    B --> C[Backend Zips Files]
    C --> D[Streams .zip back to Client]
    D --> E[Client Saves bulk-download.zip]
```

#### ğŸ§ª Run Script

```bash
# Download selected files from a bucket as a zip archive
node bulk-download.js
```

> Ensure that the `fileIds` array and JWT token are correctly populated.

---

### ğŸ“‚ Code Location Reference

| Example                 | File                                               |
| ----------------------- | -------------------------------------------------- | 
| ğŸ“¦ Bulk Download Script | [`bulk-download.js`](./bulk-download.js)           |

---

## ğŸ” Security Features

### Authentication & Authorization

```mermaid
graph LR
    A[Client Request] --> B{JWT Token Valid?}
    B -->|No| C[401 Unauthorized]
    B -->|Yes| D{ACL Check}
    D -->|Denied| E[403 Forbidden]
    D -->|Allowed| F[Process Request]
    F --> G[Return Response]
    
    style A fill:#e1f5fe
    style C fill:#ffcdd2
    style E fill:#ffcdd2
    style G fill:#c8e6c9
```

### Security Checklist

- âœ… **JWT Authentication** - Secure token-based auth
- âœ… **Password Hashing** - bcrypt with salt rounds
- âœ… **Input Validation** - Joi schema validation
- âœ… **SQL Injection Prevention** - Mongoose ODM protection
- âœ… **CORS Configuration** - Controlled cross-origin requests
- âœ… **Rate Limiting** - Express rate limiter middleware
- âœ… **HTTPS Enforcement** - NGINX SSL termination
- âœ… **File Type Validation** - Whitelist allowed file types
- âœ… **Access Control Lists** - Granular permissions
- âœ… **Secure Headers** - Helmet.js security headers

### ACL Permission Matrix

| Resource Type | Owner | Write | Read | Public |
|---------------|-------|--------|------|--------|
| **Create** | âœ… | âŒ | âŒ | âŒ |
| **Read** | âœ… | âœ… | âœ… | âœ…* |
| **Update** | âœ… | âœ… | âŒ | âŒ |
| **Delete** | âœ… | âŒ | âŒ | âŒ |
| **Share** | âœ… | âœ… | âŒ | âŒ |
| **Version Control** | âœ… | âŒ | âŒ | âŒ |

*Public access only via share links

---

Perfect â€” here's a **complete Docker Deployment section** for your `README.md` that includes:

* âœ… A professional technical breakdown of services
* ğŸ—ºï¸ A Mermaid diagram to show container interactions
* ğŸ§° A `Makefile` to simplify Docker commands
* ğŸ“¦ All deployment info (dev and prod)
* ğŸ›‘ Kubernetes references removed

---

## ğŸ³ Docker Deployment 

This project uses **Docker Compose** for managing multi-service infrastructure, enabling quick bootstrapping in both development and production environments.

---

### ğŸ§± Architecture Overview

```mermaid
flowchart LR
    subgraph Client
        A[ğŸ§‘â€ğŸ’» Browser / API Consumer]
    end

    subgraph Docker Network
        B[ğŸŒ NGINX<br/>:443 / :80] --> C[ğŸš€ SCS App<br/>:3000]
        C --> D[(ğŸ—ƒï¸ MongoDB)]
        C --> E[(âš¡ Redis)]
        C --> F[(ğŸ“¨ Kafka)]

        classDef service fill:#E3F2FD,stroke:#90CAF9
        class B,C,D,E,F service
    end

    A --> B
```

---

### âš™ï¸ Development Setup (`docker-compose.dev.yml`)

Use this setup when you're actively developing the project.

#### ğŸ”§ Services:

| Service | Role                                 | Port(s) | Persistent Volume  |
| ------- | ------------------------------------ | ------- | ------------------ |
| `app`   | Node.js backend server               | 3000    | Yes (mounted code) |
| `mongo` | MongoDB instance                     | 27017   | âœ… `mongo_data`     |
| `redis` | In-memory caching                    | 6379    | âœ… `redis_data`     |
| `kafka` | Event streaming (Bitnami KRaft mode) | 9092    | âœ… `kafka_data`     |
| `nginx` | HTTPS reverse proxy with SSL         | 80, 443 | âŒ                  |

#### ğŸ”‘ Custom Files:

* SSL certs: `./src/services/nginx/ssl/selfsigned.crt`, `.key`
* Config: `./src/services/nginx/nginx.conf`

---

### ğŸš€ Production Deployment

#### ğŸ“¦ 1. Build Docker Image

```bash
docker build -t scs:latest .
```

#### ğŸ” 2. Launch Using Production Compose

```bash
docker-compose -f docker-compose.prod.yml up -d
```

#### âš–ï¸ 3. Scale the App (Optional)

```bash
docker-compose -f docker-compose.prod.yml up -d --scale app=4
```

> Make sure NGINX or a load balancer routes traffic properly across scaled containers.

---

### ğŸ§° Optional: Makefile for Easier Commands

Create a `Makefile` in the root directory:

```Makefile
# Makefile

# Development
dev-up:
	docker-compose -f docker-compose.dev.yml up -d

dev-down:
	docker-compose -f docker-compose.dev.yml down

# Production
prod-build:
	docker build -t scs:latest .

prod-up:
	docker-compose -f docker-compose.prod.yml up -d

prod-scale:
	docker-compose -f docker-compose.prod.yml up -d --scale app=4

prod-down:
	docker-compose -f docker-compose.prod.yml down

logs:
	docker-compose logs -f --tail=100
```

#### ğŸ“¦ Usage

```bash
make dev-up        # Start dev stack
make prod-up       # Run production stack
make logs          # Tail logs from all services
```

---

### ğŸ“š Environment Configuration

Ensure your `.env` (or `.env.example`) includes:

```env
NODE_ENV=development
PORT=3000
MONGOOSE_URI=mongodb://mongo:27017/scs
REDIS_URL=redis://redis:6379
JWT_SECRET=your-secret-key
```

---

## ğŸ“Š Monitoring & Analytics

### Performance Metrics Dashboard

```mermaid
graph TB
    subgraph "Application Metrics"
        A[Request Rate<br/>req/sec]
        B[Response Time<br/>ms]
        C[Error Rate<br/>%]
        D[Active Connections<br/>count]
    end
    
    subgraph "System Metrics"
        E[CPU Usage<br/>%]
        F[Memory Usage<br/>MB]
        G[Disk I/O<br/>MB/s]
        H[Network I/O<br/>MB/s]
    end
    
    subgraph "Business Metrics"
        I[Files Uploaded<br/>count]
        J[Storage Used<br/>GB]
        K[Active Users<br/>count]
        L[Download Requests<br/>count]
    end
    
    style A fill:#e8f5e8
    style B fill:#e8f5e8
    style C fill:#ffcdd2
    style D fill:#e8f5e8
```

### Kafka Event Analytics

```javascript
// kafka-consumer-analytics.js
const { Kafka } = require('kafkajs');

const kafka = Kafka({
  clientId: 'analytics-consumer',
  brokers: ['localhost:9092']
});

const consumer = kafka.consumer({ groupId: 'analytics-group' });

async function startAnalytics() {
  await consumer.connect();
  await consumer.subscribe({ topic: 'file-events' });
  
  await consumer.run({
    eachMessage: async ({ topic, partition, message }) => {
      const event = JSON.parse(message.value.toString());
      
      // Process different event types
      switch (event.event) {
        case 'upload':
          await trackUpload(event);
          break;
        case 'download':
          await trackDownload(event);
          break;
        case 'share':
          await trackShare(event);
          break;
        case 'delete':
          await trackDelete(event);
          break;
      }
    },
  });
}

async function trackUpload(event) {
  console.log(`ğŸ“ File uploaded: ${event.filename} (${event.size} bytes)`);
  // Update analytics database
  // Send to monitoring service
}

startAnalytics().catch(console.error);
```

### Health Check Endpoint

```javascript
// health-check.js
app.get('/health', async (req, res) => {
  const health = {
    status: 'OK',
    timestamp: new Date().toISOString(),
    uptime: process.uptime(),
    environment: process.env.NODE_ENV,
    version: process.env.npm